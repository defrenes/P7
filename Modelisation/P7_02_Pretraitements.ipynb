{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0105169c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"text-align:center;\">\n",
    "Formation Data Scientist - Projet n°7 - OpenClassrooms - Christoph Pruvost - avril/mai 2022\n",
    "    \n",
    "<center><a href=\"https://openclassrooms.com/fr/paths/164/projects/632/assignment\">https://openclassrooms.com/fr/paths/164/projects/632/assignment</a></center>\n",
    "<!-- <p></p>\n",
    "<ul class=\"last simple\">\n",
    "<li></li> -->\n",
    "</div>\n",
    "\n",
    "# Implémentez un modèle de scoring\n",
    "<div class=\"admonition note alert alert-info\">\n",
    "    <p class=\"first admonition-title\" style=\"font-size:25px;font-weight:bold;text-align:center\">Prétraitements</p>\n",
    "<p class=\"last\">&nbsp;\n",
    "<p>Nous sommes Data Scientist au sein d'une société financière, nommée <strong>\"Prêt à dépenser\"</strong>, &nbsp;qui propose des crédits à la consommation pour des personnes ayant peu ou pas du tout d'historique de prêt.</p>\n",
    "\n",
    "<p>L’entreprise souhaite <strong>mettre en œuvre un outil de “scoring crédit” pour calculer la probabilité </strong>qu’un client rembourse son crédit, puis classifie la demande en crédit accordé ou refusé. Elle souhaite donc développer un <strong>algorithme de classification</strong> en s’appuyant sur des sources de données variées (données comportementales, données provenant d'autres institutions financières, etc.).</p>\n",
    "<p>De plus, les chargés de relation client ont fait remonter le fait que les clients sont de plus en plus demandeurs de <strong>transparence</strong> vis-à-vis des décisions d’octroi de crédit. Cette demande de transparence des clients va tout à fait dans le sens des valeurs que l’entreprise veut incarner.</p>\n",
    "<p><strong>Prêt à dépenser </strong>décide donc de <strong>développer un dashboard interactif</strong> pour que les chargés de relation client puissent à la fois expliquer de façon la plus transparente possible les décisions d’octroi de crédit, mais également permettre à leurs clients de disposer de leurs informations personnelles et de les explorer facilement.</p>\n",
    "<br><u><b>Les données</b></u>\n",
    "<p><a href=\"https://www.kaggle.com/c/home-credit-default-risk/data\">Voici les données</a> dont nous aurons besoin pour réaliser le dashboard. Pour plus de simplicité, nous pouvons également les télécharger à <a href=\"https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/Parcours_data_scientist/Projet+-+Impl%C3%A9menter+un+mod%C3%A8le+de+scoring/Projet+Mise+en+prod+-+home-credit-default-risk.zip\">cette adresse</a>.</p>\n",
    "<br><u><b>Notre mission</b></u>\n",
    "<ol>\n",
    "<li>Construire un modèle de scoring qui donnera une prédiction sur la probabilité de faillite d'un client de façon automatique.</li>\n",
    "<li>Construire un dashboard interactif à destination des gestionnaires de la relation client permettant d'interpréter les prédictions faites par le modèle, et d’améliorer la connaissance client des chargés de relation client.</li>\n",
    "</ol>\n",
    "<p><strong>Michaël</strong>, notre manager, nous incite à sélectionner un kernel Kaggle pour nous faciliter la préparation des données nécessaires à l’élaboration du modèle de scoring. Nous analyserons ce kernel et l’adapterons pour nous assurer qu’il répond aux besoins de votre mission. Nous pourrons ainsi nous focaliser sur l’élaboration du modèle, son optimisation et sa compréhension.</p>\n",
    "&nbsp;\n",
    "<p>Nous allons réaliser ici les <strong>prétraitements des données</strong>.&nbsp;\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "*Le prétraitement s'appuie sur le kernel Kaggle suivant :* https://www.kaggle.com/code/jsaguiar/lightgbm-with-simple-features/script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c52bd74",
   "metadata": {},
   "source": [
    "## Imports et variables\n",
    "Importons les librairies nécessaires à notre analyse et définissons quelques variables globales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2dc58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, datetime, sys, os, gc, re\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a311f9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version des librairies utilisées le 2022-06-06 09:13:20.271157 \n",
      " Python : 3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:38:57) [GCC 10.3.0] \n",
      " NumPy : 1.22.3 \n",
      " Pandas : 1.4.2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versions des librairies utilisées :\n",
    "print('Version des librairies utilisées le', datetime.datetime.now(), '\\n',\n",
    "       'Python :', sys.version, '\\n',\n",
    "       'NumPy :',  np.version.full_version, '\\n',\n",
    "       'Pandas :', pd.__version__, '\\n',\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2d93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquons certaines options globales Pandas et seaborn :\n",
    "pandas_options = {\n",
    "    'display.max_rows': 400,\n",
    "    'display.max_column': 50,\n",
    "    'display.width': 100,\n",
    "    'display.max_colwidth': 2000,  # 200\n",
    "    'display.float_format': '{:,.2f}'.format # arrondi à 2 chiffres après la virgule\n",
    "}\n",
    "\n",
    "for cle, valeur in pandas_options.items():\n",
    "    pd.set_option(cle, valeur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9aa5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "data_dir = '.data'\n",
    "fichier_donnees_pretraitees = \"donnees_pretraitees.ftr\" # Pour la sauvegarde à venir.\n",
    "# debug = True # réduit le nombre d'individus sélectionnés dans le jeu de données.\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972a4749",
   "metadata": {},
   "source": [
    "## Définition de fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d0f51",
   "metadata": {},
   "source": [
    "### Timer\n",
    "Définissions une fonction qui va nous permettre de mesurer le temps pris par l'exécution des différentes parties du code principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef737eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - Fait en {:.0f}s\".format(title, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f22ac",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding\n",
    "Fonction de One-Hot-Encoding des variables qualitatives, nécessaire au pré-traitement des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88e5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c19880",
   "metadata": {},
   "source": [
    "### Pré-traitements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be910ead",
   "metadata": {},
   "source": [
    "#### Données d'entraînement et de test\n",
    "1. Chargement et concaténation des données d'entraînement et de test,\n",
    "1. Suppression de 4 valeurs aberrantes,\n",
    "1. Encodage manuel des variables qualitatives avec 2 modes (→ 0 et 1),\n",
    "1. Encodage One-Hot des des variables qualitatives restantes,\n",
    "1. Remplacement de valeurs aberrantes de `DAYS_EMPLOYED` (365.243 → NaN),\n",
    "1. Feature Engineering :\n",
    " 1. `DAYS_EMPLOYED_PERC` : nombre de jours passés dans l'emploi actuel par rapport à l'age,\n",
    " 1. `INCOME_CREDIT_PERC` : montant total des revenus par rapport au montant du prêt,\n",
    " 1. `INCOME_PER_PERSON` : montant total des revenus par rapport au nombre de personnes dans le foyer,\n",
    " 1. `ANNUITY_INCOME_PERC` : montant des annuités divisé par le montant total de revenu,\n",
    " 1. `PAYMENT_RATE` : montant des annuité divisé par le montant du crédit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95df3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-traitement des données d'entraînement et de test: application_train.csv et application_test.csv\n",
    "\n",
    "def application_train_test(num_rows = None, nan_as_category = False):\n",
    "    # Ouverture des fichiers de données et concaténation\n",
    "    df = pd.read_csv(os.path.join(data_dir, 'application_train.csv'), nrows= num_rows)\n",
    "    test_df = pd.read_csv(os.path.join(data_dir, 'application_test.csv'), nrows= num_rows)\n",
    "    print(\"Nb données d'entraînement : {}, Nb données de test : {}\".format(len(df), len(test_df)))\n",
    "    df = pd.concat([df, test_df], ignore_index=True)\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    # Some simple new features (percentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681f9233",
   "metadata": {},
   "source": [
    "#### bureau.csv et bureau_balance.csv\n",
    "1. Chargement des fichiers `bureau.csv` et `bureau_balance.csv`,\n",
    "1. Encodage One-Hot des variables qualitatives des deux fichiers,\n",
    "1. Feature Engineering :\n",
    " 1. Création de variables pour bureau_balance.csv : \n",
    "    1. min, max, size pour `MONTHS_BALANCE`,\n",
    "    1. mean pour toutes les colonnes créées par encodage One-Hot,\n",
    "    1. ajout des variables aux données de `bureau.csv`,\n",
    " 1. Création de variables pour le jeu de données aggrégées,\n",
    "    1. Nouvelles variables pour les données qualitatives,\n",
    "    1. Nouvelles variables pour les données quantitatives,\n",
    "       1. pour les crédits en cours,\n",
    "       1. pour les crédits passés,\n",
    "1. Suppression de la variable `SK_ID_BUREAU` devenue inutile après l'aggrégation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7f7efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(num_rows = None, nan_as_category = True):\n",
    "    bureau = pd.read_csv(os.path.join(data_dir, 'bureau.csv'), nrows = num_rows)\n",
    "    bb = pd.read_csv(os.path.join(data_dir, 'bureau_balance.csv'), nrows = num_rows)\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a4b01a",
   "metadata": {},
   "source": [
    "#### previous_applications.csv\n",
    "1. Chargement du fichier `previous_application.csv`,\n",
    "1. Encodage One-Hot des données qualitatives,\n",
    "1. Remplacement pour plusieurs variables des valeurs aberrantes du nombre de jours (365 243 → NaN),\n",
    "1. Feature Engineering :\n",
    " 1. montant demandé par rapport au montant reçu,\n",
    " 1. min, max, moyenne des variables quantitatives,\n",
    "     1. pour l'ensemble des demandes de prêt,\n",
    "     1. pour les prêts accordés uniquements,\n",
    "     1. pour les prêts refusés uniquement,\n",
    " 1. moyenne des variables qualitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e25765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(num_rows = None, nan_as_category = True):\n",
    "    prev = pd.read_csv(os.path.join(data_dir, 'previous_application.csv'), nrows = num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af725277",
   "metadata": {},
   "source": [
    "### previous_applications.csv\n",
    "1. Chargement du fichier `POS_CASH_balance.csv`,\n",
    "1. Encodage One-Hot des variables qualitatives,\n",
    "1. Feature Engineering :\n",
    " 1. Variables quantitatives : moyenne, max et taille de certaines variables,\n",
    " 1. Variables qualitatives : moyenne,\n",
    " 1. Taille d'un point de vente donné en nombre de dossiers,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11904b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(num_rows = None, nan_as_category = True):\n",
    "    pos = pd.read_csv(os.path.join(data_dir, 'POS_CASH_balance.csv'), nrows = num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2660b66",
   "metadata": {},
   "source": [
    "### installments_payments.csv\n",
    "1. Chargement du fichier `installments_payments.csv`,\n",
    "1. Encodage One-Hot des données qualitatives,\n",
    "1. Feature Engineering\n",
    " 1. Variables quantitatives \n",
    "     1. Montant payé par le client par rapport au montant de la traite,\n",
    "     1. Différence entre la traite et le montant payé par le client,\n",
    "     1. Nombre de jours de dépassement du remboursement du dernier prêt, le cas échéant,\n",
    "     1. Nombre de jours restants avant rembrousement du dernier prêt, le cas échéant,\n",
    "     1. Max, moyenne, somme, variance, nombre, pour différentes variables selon les cas,\n",
    " 1. Variables qualitatives \n",
    "     1. Moyenne\n",
    " 1. Nombre de versements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd5e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows = None, nan_as_category = True):\n",
    "    ins = pd.read_csv(os.path.join(data_dir, 'installments_payments.csv'), nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954b666e",
   "metadata": {},
   "source": [
    "### credit_card_balance.csv\n",
    "1. Chargement du fichier `credit_card_balance.csv`\n",
    "1. Encodage One-Hot des variables qualitatives,\n",
    "1. Feature Engineering :\n",
    " 1. Variables quatitatives : min, max, moyenne, somme, variance\n",
    " 1. Nombre d'opérations par client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bcecc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(num_rows = None, nan_as_category = True):\n",
    "    cc = pd.read_csv(os.path.join(data_dir, 'credit_card_balance.csv'), nrows = num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b4041",
   "metadata": {},
   "source": [
    "## Chaîne de traitement principale\n",
    "Nous définissons ici la chaîne de traitement principale qui utilise les fonctions définies précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "417d3149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(debug = False):\n",
    "    num_rows = 10000 if debug else None\n",
    "    print('─'*50)\n",
    "    with timer(\"Pré-traitement des fichiers application_train.csv et application_test.csv\"):\n",
    "        print(\"application_train.csv et application_test.csv\")\n",
    "        df = application_train_test(num_rows)\n",
    "        print(\"Dimensions du jeu de données :\", df.shape)        \n",
    "    with timer(\"Pré-traitement des fichiers bureau.csv et bureau_balance.csv\"):\n",
    "        print('─'*50)\n",
    "        print(\"bureau.csv et bureau_balance.csv\")\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Dimensions du jeu de données :\", bureau.shape)\n",
    "        print(\"Intégration des données au jeu de données principal\")\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Pré-traitement du fichier previous_applications.csv\"):\n",
    "        print('─'*50)\n",
    "        print(\"previous_applications.csv\")\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Dimensions du jeu de données :\", prev.shape)\n",
    "        print(\"Intégration des données au jeu de données principal\")\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Pré-traitement du fichier POS-CASH_balance.csv\"):\n",
    "        print('─'*50)\n",
    "        print(\"POS-CASH_balance.csv\")\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Dimensions du jeu de données résultant :\", pos.shape)\n",
    "        print(\"Intégration des données au jeu de données principal\")\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Pré-traitement du fichier installments_payments.csv\"):\n",
    "        print('installments_payments.csv')\n",
    "        print('─'*50)\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Dimensions du jeu de données résultant :\", ins.shape)\n",
    "        print(\"Intégration des données au jeu de données principal\")\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Pré-traitement du fichier credit_card_balance.csv\"):\n",
    "        print('─'*50)\n",
    "        print('credit_card_balance.csv')\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Dimensions du jeu de données résultant :\", cc.shape)\n",
    "        print(\"Intégration des données au jeu de données principal\")\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b5eaf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "──────────────────────────────────────────────────\n",
      "application_train.csv et application_test.csv\n",
      "Nb données d'entraînement : 307511, Nb données de test : 48744\n",
      "Dimensions du jeu de données : (356251, 247)\n",
      "Pré-traitement des fichiers application_train.csv et application_test.csv - Fait en 5s\n",
      "──────────────────────────────────────────────────\n",
      "bureau.csv et bureau_balance.csv\n",
      "Dimensions du jeu de données : (305811, 116)\n",
      "Intégration des données au jeu de données principal\n",
      "Pré-traitement des fichiers bureau.csv et bureau_balance.csv - Fait en 23s\n",
      "──────────────────────────────────────────────────\n",
      "previous_applications.csv\n",
      "Dimensions du jeu de données : (338857, 249)\n",
      "Intégration des données au jeu de données principal\n",
      "Pré-traitement du fichier previous_applications.csv - Fait en 28s\n",
      "──────────────────────────────────────────────────\n",
      "POS-CASH_balance.csv\n",
      "Dimensions du jeu de données résultant : (337252, 18)\n",
      "Intégration des données au jeu de données principal\n",
      "Pré-traitement du fichier POS-CASH_balance.csv - Fait en 17s\n",
      "installments_payments.csv\n",
      "──────────────────────────────────────────────────\n",
      "Dimensions du jeu de données résultant : (339587, 26)\n",
      "Intégration des données au jeu de données principal\n",
      "Pré-traitement du fichier installments_payments.csv - Fait en 32s\n",
      "──────────────────────────────────────────────────\n",
      "credit_card_balance.csv\n",
      "Dimensions du jeu de données résultant : (103558, 141)\n",
      "Intégration des données au jeu de données principal\n",
      "Pré-traitement du fichier credit_card_balance.csv - Fait en 19s\n",
      "Traitement intégral - Fait en 125s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Traitement intégral\"):\n",
    "    df = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0daa22",
   "metadata": {},
   "source": [
    "## Traitement des valeurs aberrantes et manquantes\n",
    "Nous allons traiter ici les valeurs aberrantes et manquantes restantes après les traitements précédents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39241a90",
   "metadata": {},
   "source": [
    "### Valeurs aberrantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc723ea",
   "metadata": {},
   "source": [
    "#### Valeurs infinies\n",
    "Traitons les valeurs infnies du jeu de données en les remplaçant par des NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ec78685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PREV_APP_CREDIT_PERC_MAX        1\n",
       "REFUSED_APP_CREDIT_PERC_MAX     1\n",
       "INSTAL_PAYMENT_PERC_MAX        19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inf = np.isinf(df).sum()\n",
    "val_inf = val_inf[val_inf > 0]\n",
    "val_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8c0ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in val_inf.index :\n",
    "    df[col] = df[col].replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f2258",
   "metadata": {},
   "source": [
    "### Valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86e16910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TARGET                               48744\n",
       "AMT_ANNUITY                             36\n",
       "AMT_GOODS_PRICE                        278\n",
       "DAYS_EMPLOYED                        64648\n",
       "OWN_CAR_AGE                         235239\n",
       "                                     ...  \n",
       "CC_NAME_CONTRACT_STATUS_nan_MAX     252693\n",
       "CC_NAME_CONTRACT_STATUS_nan_MEAN    252693\n",
       "CC_NAME_CONTRACT_STATUS_nan_SUM     252693\n",
       "CC_NAME_CONTRACT_STATUS_nan_VAR     253385\n",
       "CC_COUNT                            252693\n",
       "Length: 617, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAN = df.isna().sum()\n",
    "NAN[ NAN > 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e0e7a",
   "metadata": {},
   "source": [
    "### Valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05065b2e",
   "metadata": {},
   "source": [
    "#### Suppression des variables avec plus de 70% de NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2edfbcd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356251, 797)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copie de sauvegarde au cas où\n",
    "df_backup = df.copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c47f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des colonnes avec plus de 70% de NaN \n",
    "df.dropna(thresh=0.7*df.shape[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "671457ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356251, 544)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34516bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nombre de variables supprimées :\n",
    "798-545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "962f6bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   1.00\n",
       "Name: TARGET, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"TARGET\"].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58432805",
   "metadata": {},
   "source": [
    "#### Imputations\n",
    "Sélectionnons toutes les variables avec des valeurs manquantes, en dehors de la variable cible et procédons à une imputation par la médiane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "add40006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_imputables = [var for var in df.columns if var != 'TARGET' and df[var].isna().any()]\n",
    "len(var_imputables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb107633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation par la médiane\n",
    "for variable in var_imputables:\n",
    "    df[variable].fillna(value=df[variable].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89039fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TARGET    48744\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAN = df.isna().sum()\n",
    "NAN[ NAN > 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b5d39",
   "metadata": {},
   "source": [
    "### Renommage des variables\n",
    "Certains caractères utilisés pour les noms de variables posent problème lors de l'utilisation de LightGBM. Nous allons donc renommer les variables pour supprimer les caractères problématiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f63d9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renommage des colonnes pour corriger un problème avec lightgbm :\n",
    "# lightgbm.basic.LightGBMError: Do not support special JSON characters in feature name.\n",
    "# https://stackoverflow.com/questions/60582050/lightgbmerror-do-not-support-special-json-characters-in-feature-name-the-same#62364946\n",
    "df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_ ]+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb8c31c",
   "metadata": {},
   "source": [
    "## Sauvegarde des données\n",
    "Sauvegardons le résultat du pré-traitement afin de pouvoir y accéder pour la modélisation. Les données sauvegardées contiennent à la fois les données d'entraînement et les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e861b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du jeu de données intégral (données d'entraînement et de test kaggle)\n",
    "df.reset_index(drop=True).to_feather(os.path.join(data_dir, fichier_donnees_pretraitees))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flask",
   "language": "python",
   "name": "flask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Sommaire",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "202.017px",
    "left": "1395.67px",
    "right": "20px",
    "top": "115px",
    "width": "359.333px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
